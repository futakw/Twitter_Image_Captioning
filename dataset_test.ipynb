{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "\n",
    "from collect_twitter_data.data_info import data_info\n",
    "use_account = data_info['animal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img_transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils\n",
    "from torchvision import models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "def loadPickle(fileName):\n",
    "    with open(fileName, mode=\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, split, use_account, vocab, image_transform=None, text_tokenizer=None, data_dir='data'):\n",
    "        self.split = split\n",
    "        self.image_transform = image_transform\n",
    "#         self.text_transform = text_transform\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.data_dir =  data_dir\n",
    "#         self.imgs = glob.glob(os.path.join(self.data_dir, \"resized_images/*.png\"))\n",
    "        self.imgs = glob.glob(os.path.join(self.data_dir, \"images/*.png\"))\n",
    "\n",
    "        # 使うtwitterアカウントのアノテーションだけ読み込む\n",
    "        self.annos = []\n",
    "        for user in use_account:\n",
    "            ann_path = os.path.join(self.data_dir, f\"annos/{user}.pickle\")\n",
    "            ann = loadPickle(ann_path)\n",
    "            self.annos += ann\n",
    "\n",
    "        print(f'Created {self.split} Dataset of Len: {len(self.annos)}')\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annos[idx]\n",
    "        image_file = os.path.join(self.data_dir, f'images/{ann[\"filename\"]}')\n",
    "\n",
    "        orig_text = ann['text']\n",
    "        orig_img = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "        img = self.image_transform[self.split](orig_img)\n",
    "#         img = self.image_transform(orig_img)\n",
    "        tokens = self.text_tokenizer.tokenize(orig_text, return_str=True).split()\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<start>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "\n",
    "#         data = {'image': img, 'text': text, 'orig_img': orig_img, 'orig_text': orig_text ,'screen_name': ann['screen_name']}\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.japanese_tokenizer import JapaneseTokenizer\n",
    "from build_vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_dict_path = \"/home/smg/nishikawa/src/lib/mecab/dic/ipadic\"\n",
    "text_tokenizer = JapaneseTokenizer(splitter=\"MeCab\", model=mecab_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/vocab_ja.pkl', 'rb') as f:\n",
    "#     vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/home/smg/nishikawa/pytorch-tutorial/tutorials/03-advanced/image_captioning/data/vocab_ja.pkl\", 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train Dataset of Len: 343\n"
     ]
    }
   ],
   "source": [
    "dataset = TwitterDataset(split, use_account, vocab, image_transform=image_transform, text_tokenizer=text_tokenizer, data_dir='data')\n",
    "# dataset = TwitterDataset(split, use_account, image_transform=transform, data_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'screen_name': 'mofumofu_cn',\n",
       " 'text': 'えへへ〜どうだ〜w ',\n",
       " 'media_url': 'http://pbs.twimg.com/media/B2jQH86CIAAfwJ-.jpg',\n",
       " 'media_id': 533905390870601728,\n",
       " 'filename': 'mofumofu_cn_533905390870601728.png'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.annos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                          batch_size=16,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 19, 16, 11, 10, 9, 9, 9, 9, 9, 8, 7, 6, 6, 5, 4]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq                        # heapq のインポート\n",
    "hq = []                             # heapqの作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heappush(hq, (1, 2))     # 要素の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heapq.heappop(hq)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heappush(hq, (1, [2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [2, 3]), (9, 2)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heapq.heappop(hq)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heapq.heappop(hq)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
